KULLANILABİLECEK YAPAY ZEKA MODELLERİ:
================================================
                   KAPTAN
https://huggingface.co/OpenGVLab/InternVL3_5-38B-Flash/tree/main


https://huggingface.co/barandinho/aya-expanse-32b-turkish-reasoning-sft-ckpt5
https://huggingface.co/malhajar/Mixtral-8x7B-v0.1-turkish
================================================
         KODCU, DOSYACI
https://github.com/QwenLM/Qwen3-Coder (kod müstahdemi, kod tenkitçi, kod tavsiyeci, dosya tipi çevirici)
https://huggingface.co/DavidAU/Qwen2.5-Microsoft-NextCoder-Brainstorm20x-42B (kod müstahdemi, kod tenkitçi, kod tavsiyeci, dosya tipi çevirici)
https://huggingface.co/Dream-org/Dream-Coder-v0-Base-7B
https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Reasoning
================================================
             HAFIZA, SUALCİ, MUHABERE
https://huggingface.co/THU-KEG/LongWriter-Zero-32B (Hafıza, süreç ikazcı, süreç tenkitçi, süreç tavsiyeci, vakitçi)

https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-4194k (Hafıza, süreç ikazcı, süreç tenkitçi, vakitçi)


https://huggingface.co/sambanovasystems/SambaLingo-Turkish-Base/tree/main 

https://huggingface.co/ytu-ce-cosmos/Turkish-Gemma-9b-T1/tree/main ✅ (Muhaberatçı, sualci)
===============================================
 https://huggingface.co/NX-AI/xLSTM-7b
https://huggingface.co/meta-llama/Llama-Prompt-Guard-2-86M
https://github.com/michaelhodel/re-arc

https://github.com/marbetschar/arc-prize-2024/blob/main/DreamCoder.ipynb
https://github.com/marbetschar/arc-prize-2024/blob/main/arc2024/data.py
https://github.com/marbetschar/arc-dsl/tree/aaaa2d78bb17978ae1fe6b266f7bb27863b441fc

https://github.com/mxbi/dreamcoder-arc?tab=readme-ov-file
https://github.com/victorvikram/ARC-icecuber
https://github.com/mxbi/dreamcoder-arc

https://github.com/jerber/arc-lang-public/blob/main/src/run.py
https://dspy.ai

https://github.com/jwzhanggy/Graph_Toolformer/tree/main
https://huggingface.co/facebook/llm-compiler-13b-ftd
https://github.com/google-research-datasets/Education-Dialogue-Dataset
https://huggingface.co/datasets/eth-nlped/mathdial
https://huggingface.co/datasets/Eedi/Question-Anchored-Tutoring-Dialogues-2k/viewer/dq-question-metadata
https://www.kaggle.com/datasets/rtatman/questionanswer-dataset
https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset
https://www.kaggle.com/datasets/thedevastator/open-source-logical-reasoning-dataset
https://huggingface.co/datasets/hivaze/LOGIC-701/viewer
https://www.kaggle.com/datasets/thedevastator/logical-reasoning-improvement-dataset
https://www.kaggle.com/datasets/thedevastator/logical-reasoning-improvement-dataset
https://www.kaggle.com/datasets/sudeepkharel/deep-maths

https://huggingface.co/docs/transformers/en/model_doc/kosmos-2
https://huggingface.co/docs/transformers/en/model_doc/llava

https://github.com/logic-and-learning-lab/Popper
https://github.com/metagol/metagol
https://github.com/potassco/clingo
https://github.com/ilaspltd/ILASP-releases/releases/tag/v4.4.1

https://huggingface.co/Qwen/Qwen3-32B
https://huggingface.co/Qwen/Qwen3-Embedding-8B
https://huggingface.co/unsloth/gpt-oss-20b-BF16



from sentence_transformers import SentenceTransformer

model = SentenceTransformer("intfloat/multilingual-e5-large")

sentences = [
    "The weather is lovely today.",
    "It's so sunny outside!",
    "He drove to the stadium."
]
embeddings = model.encode(sentences)

similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]


Remote Inference via Inference Providers

Ensure you have a valid HF_TOKEN set in your environment. You can get your token from your settings page. Note: running this may incur charges above the free tier. The following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you. For more information on how to use the Inference Providers, please refer to our documentation and guides.

import os
os.environ['HF_TOKEN'] = 'YOUR_TOKEN_HERE'

import os
from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="auto",
    api_key=os.environ["HF_TOKEN"],
)

result = client.feature_extraction(
    "Today is a sunny day and I will get some ice cream.",
    model="intfloat/multilingual-e5-large",
)

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="TURKCELL/Turkcell-LLM-7b-v1")
messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe(messages)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("TURKCELL/Turkcell-LLM-7b-v1")
model = AutoModelForCausalLM.from_pretrained("TURKCELL/Turkcell-LLM-7b-v1")
messages = [
    {"role": "user", "content": "Who are you?"},
]
inputs = tokenizer.apply_chat_template(
	messages,
	add_generation_prompt=True,
	tokenize=True,
	return_dict=True,
	return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))